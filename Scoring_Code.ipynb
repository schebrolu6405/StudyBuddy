{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scoring Technique\n",
        "\n",
        "* BLEU\n",
        "* Rouge\n",
        "* BERT Score\n",
        "* GPT Score"
      ],
      "metadata": {
        "id": "sq2-GGJy4bwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "VxvkC6-d4sHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score"
      ],
      "metadata": {
        "id": "OAQv_aLR4sEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "KPpAPT_y4sKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from bert_score import score as bertscore\n",
        "import openai\n",
        "from typing import List\n",
        "\n",
        "# Optional: set your OpenAI key if you're using GPT-based scoring\n",
        "#openai.api_key = \"your-openai-api-key\"\n",
        "import evaluate\n",
        "\n",
        "# Load metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "ground_truth_responses = [\"they used the following dataset: ImageNet, CIFAR-10, MS COCO and Pascal VOC 2007 and 2012\"]"
      ],
      "metadata": {
        "id": "LlZbCNTG4sNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update your model response here\n",
        "generated_responses = [response['response']]"
      ],
      "metadata": {
        "id": "Se-O4p8h5yn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === ROUGE ===\n",
        "#rouge = load_metric(\"rouge\")\n",
        "rouge_scores = rouge.compute(predictions=generated_responses, references=ground_truth_responses)\n",
        "print(\"ROUGE scores:\\n\", rouge_scores)\n"
      ],
      "metadata": {
        "id": "VOuT0tZM5IHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === BLEU ===\n",
        "bleu_score = bleu.compute(predictions=generated_responses, references=ground_truth_responses)\n",
        "print(\"BLEU Score:\\n\", bleu_score)"
      ],
      "metadata": {
        "id": "retuzC6x5JFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "client = openai.OpenAI(api_key=\"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\")\n",
        "\n",
        "def gpt_score_response(prompt: str, response: str, reference: str):\n",
        "    system_prompt = (\n",
        "        \"You're an expert evaluator. Given a user query, a model response, and a reference answer, \"\n",
        "        \"rate the model response for helpfulness, relevance, and accuracy on a scale of 1-10 each. \"\n",
        "        \"Return a JSON with keys: helpfulness, relevance, accuracy.\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"\n",
        "Query: {prompt}\n",
        "Model Response: {response}\n",
        "Reference Answer: {reference}\n",
        "\"\"\"}\n",
        "    ]\n",
        "\n",
        "    chat_response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages,\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    return chat_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "xDP75pok5TMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Paper - pdf_url = 'https://arxiv.org/pdf/1512.03385'\n",
        "#### Resnet #####\n",
        "\n",
        "query = \"What datasets were used here?\"\n",
        "eval_result = gpt_score_response(query, generated_responses[0], ground_truth_responses[0])\n",
        "print(\"GPT-based evaluation:\\n\", eval_result)"
      ],
      "metadata": {
        "id": "DsvKJYT95TPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Paper - pdf_url = 'https://arxiv.org/pdf/1706.03762v7'\n",
        "#### Resnet #####\n",
        "query = \"What are the two main components of transformer architecture?\"\n",
        "eval_result = gpt_score_response(query, generated_responses[0], ground_truth_responses[0])\n",
        "print(\"GPT-based evaluation:\\n\", eval_result)"
      ],
      "metadata": {
        "id": "REKcGlxLCG-S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}