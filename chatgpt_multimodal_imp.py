# -*- coding: utf-8 -*-
"""ChatGPT_Multimodal_Imp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BVlSt_Pxl8oTjDWnjNItFfxcDmT9TIMT

#Multi Modal RAG using GPT-4o
* Text Summaries
* Image Summaries
* Context
* Reference to images
* Image as context
"""

!apt-get install poppler-utils tesseract-ocr

! pip install -U langchain openai langchain-chroma langchain-experimental # (newest versions required for multi-modal)

! pip install "unstructured[all-docs]" pillow pydantic lxml pillow matplotlib chromadb tiktoken

!pip install huggingface_hub[hf_xet]

!pip install pymupdf

!pip install langchain-openai

!pip install asyncio-throttle

import os

# keys for the services we will use

os.environ["OPENAI_API_KEY"] = "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"

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from unstructured.partition.pdf import partition_pdf

import requests
import tempfile
import fitz  # PyMuPDF
import re

###### Attention ####
pdf_url = 'https://arxiv.org/pdf/1706.03762v7'

#### Resnet #####
pdf_url = 'https://arxiv.org/pdf/1512.03385'

"""# **INPUT - pdf url**"""

pdf_url = ''

"""# **CODES**"""

import logging
logging.getLogger("pdfminer").setLevel(logging.ERROR)

def process_pdf_url(pdf_url, output_path="./output"):
    response = requests.get(pdf_url)
    response.raise_for_status()

    with tempfile.NamedTemporaryFile(suffix=".pdf",delete=False) as tmp_pdf:
        tmp_pdf.write(response.content)
        tmp_pdf.flush()
        pdf_path = tmp_pdf.name  # Store the file path

    doc = fitz.open(pdf_path)
    caption = []

    for page_num, page in enumerate(doc, start=1):
        blocks = page.get_text("blocks")  # get text blocks
        for block in blocks:
            text = block[4].strip()
            if re.match(r'^Figure\s*\d+', text, re.IGNORECASE):
                caption.append((page_num, text))
# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking
    chunk = partition_pdf(
      filename=pdf_path,
      infer_table_structure=True,            # extract tables
      strategy="hi_res",                     # mandatory to infer tables
      extract_image_block_types=["Image", "Table"],   # Add 'Table' to list to extract image of tables
      image_output_dir_path=output_path,   # if None, images and tables will saved in base64
      extract_image_block_to_payload=True,   # if true, will extract base64 for API usage
      chunking_strategy="by_title",          # or 'basic'
      max_characters=10000,                  # defaults to 500
      combine_text_under_n_chars=2000,       # defaults to 0
      new_after_n_chars=6000,
    # extract_images_in_pdf=True,          # deprecated
    )
    return chunk,caption

# Commented out IPython magic to ensure Python compatibility.
# %%time
# chunks,captions = process_pdf_url(pdf_url)

# separate tables from texts
tables = []
texts = []

for chunk in chunks:
    if "Table" in str(type(chunk)):
        tables.append(chunk)

    if "CompositeElement" in str(type((chunk))):
        texts.append(chunk)

# Get the images from the CompositeElement objects
def get_images_base64(chunks):
    images_b64 = []
    for chunk in chunks:
        if "CompositeElement" in str(type(chunk)):
            chunk_els = chunk.metadata.orig_elements
            for el in chunk_els:
                if "Image" in str(type(el)):
                    images_b64.append(el.metadata.image_base64)
    return images_b64

images = get_images_base64(chunks)

import base64
from IPython.display import Image, display

def display_base64_image(base64_code):
    # Decode the base64 string to binary
    image_data = base64.b64decode(base64_code)
    # Display the image
    display(Image(data=image_data))

#display_base64_image(images[0])

"""### TEXT SUMMARY"""

# Generate summaries of text elements
def generate_text_summaries(texts, tables, summarize_texts=False):
    """
    Summarize text elements
    texts: List of str
    tables: List of str
    summarize_texts: Bool to summarize texts
    """

    # Prompt
    prompt_text = """
You are an assistant tasked with summarizing tables and text.
Give a concise summary of the table or text.

Respond only with the summary, no additionnal comment.
Do not start your message by saying "Here is a summary" or anything like that.
Just give the summary as it is.

Table or text chunk: {element}

"""
    prompt = ChatPromptTemplate.from_template(prompt_text)

    # Text summary chain
    model = ChatOpenAI(temperature=0, model="gpt-4o")
    summarize_chain = {"element": lambda x: x} | prompt | model | StrOutputParser()

    # Initialize empty summaries
    text_summaries = []
    table_summaries = []

    # Apply to text if texts are provided and summarization is requested
    if texts and summarize_texts:
        text_summaries = summarize_chain.batch(texts, {"max_concurrency": 3})
    elif texts:
        text_summaries = texts

    # Apply to tables if tables are provided
    if tables:
        table_summaries = summarize_chain.batch(tables, {"max_concurrency": 3})

    return text_summaries, table_summaries


# Get text, table summaries
text_summaries, table_summaries = generate_text_summaries(
    texts, tables, summarize_texts=True
)

"""### IMAGE SUMMARY"""

from langchain_openai import ChatOpenAI
import time
from tenacity import retry, wait_random_exponential, stop_after_attempt

# Retry wrapper for rate limit handling
@retry(wait=wait_random_exponential(min=1, max=5), stop=stop_after_attempt(5))
def run_chain(chain, image_base64):
    messages = [
        (
            "user",
            [
                {"type": "text", "text": prompt_template},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{image_base64}"
                    },
                },
            ],
        )
    ]
    prompt = ChatPromptTemplate.from_messages(messages)
    single_chain = prompt | ChatOpenAI(model="gpt-4o-mini") | StrOutputParser()
    return single_chain.invoke({})


# Your prompt
prompt_template = """Describe the image in detail. For context,
the image is part of a research paper and can contain important information.
Be specific about workflow diagrams or model architectures shown in image."""

# Example loop over base64-encoded images
image_summaries = []
for image in images:  # 'images' should be a list of base64-encoded JPEG strings
    try:
        summary = run_chain(chain=None, image_base64=image)
        image_summaries.append(summary)
    except Exception as e:
        print(f"Error processing image: {e}")
        #image_summaries.append("Error in processing.")

import asyncio
from asyncio_throttle import Throttler
from langchain_openai import ChatOpenAI
import nest_asyncio
import time

# Prompt template
prompt_template = """Describe the image in detail. For context,
the image is part of a research paper and can contain important information.
Be specific about workflow diagrams or model architectures shown in image."""

# Token-safe throttler (e.g., 10 requests/sec, adjust if needed)
throttler = Throttler(rate_limit=10, period=1.0)  # 10 per second

async def describe_image_async(image_base64: str, model="gpt-4o-mini") -> str:
    async with throttler:
        prompt = ChatPromptTemplate.from_messages([
            (
                "user",
                [
                    {"type": "text", "text": prompt_template},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_base64}"
                        },
                    },
                ],
            )
        ])
        chain = prompt | ChatOpenAI(model=model) | StrOutputParser()
        return await chain.ainvoke({})

async def process_images_async(images):
    tasks = [describe_image_async(img) for img in images]
    return await asyncio.gather(*tasks, return_exceptions=True)

nest_asyncio.apply()  # Allow nested event loops in Jupyter

image_summaries = await process_images_async(images)

"""### RAG"""

import uuid
from langchain.vectorstores import Chroma
from langchain.storage import InMemoryStore
from langchain.schema.document import Document
from langchain.embeddings import OpenAIEmbeddings
from langchain.retrievers.multi_vector import MultiVectorRetriever

# The vectorstore to use to index the child chunks
vectorstore = Chroma(collection_name="multi_modal_rag", embedding_function=OpenAIEmbeddings())

# The storage layer for the parent documents
store = InMemoryStore()
id_key = "doc_id"

# The retriever (empty to start)
retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=store,
    id_key=id_key,
)

# Add texts
doc_ids = [str(uuid.uuid4()) for _ in texts]
summary_texts = [
    Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)
]
retriever.vectorstore.add_documents(summary_texts)
retriever.docstore.mset(list(zip(doc_ids, texts)))

# Add tables
if tables:
  table_ids = [str(uuid.uuid4()) for _ in tables]
  summary_tables = [
    Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)
                    ]
  retriever.vectorstore.add_documents(summary_tables)
  retriever.docstore.mset(list(zip(table_ids, tables)))

# Add image summaries
if images:
  img_ids = [str(uuid.uuid4()) for _ in images]
  summary_img = [
    Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)
                  ]
  retriever.vectorstore.add_documents(summary_img)
  retriever.docstore.mset(list(zip(img_ids, images)))

from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI
from base64 import b64decode


def parse_docs(docs):
    """Split base64-encoded images and texts"""
    b64 = []
    text = []
    for doc in docs:
        try:
            b64decode(doc)
            b64.append(doc)
        except Exception as e:
            text.append(doc)
    return {"images": b64, "texts": text}


def build_prompt(kwargs):

    docs_by_type = kwargs["context"]
    user_question = kwargs["question"]

    context_text = ""
    if len(docs_by_type["texts"]) > 0:
        for text_element in docs_by_type["texts"]:
            context_text += text_element.text

    # construct prompt with context (including images)
    prompt_template = f"""
    You are an AI assistant helping understand and summarize a research paper. Use both text and image references to answer.

    Answer the question based only on the following context, which can include text, tables. Please make sure to include all relevant images in your answer.

    Do not include references until asked.
    Context: {context_text}
    Question: {user_question}

    INSTRUCTIONS:
    - If you see image references, treat them as important visual context.
    - If an image is referenced (e.g. `[IMAGE REFERENCE: ...]`), assume it's related to diagrams, plots, or tables.
    - Provide figure number in of image reference
    -  Refer to abstarct and conclusion to summarize the document.
    - Answer the question clearly, referencing images where helpful.

    """

    prompt_content = [{"type": "text", "text": prompt_template}]

    if len(docs_by_type["images"]) > 0:
        for image in docs_by_type["images"]:
            prompt_content.append(
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image}"},
                }
            )

    return ChatPromptTemplate.from_messages(
        [
            HumanMessage(content=prompt_content),
        ]
    )


chain = (
    {
        "context": retriever | RunnableLambda(parse_docs),
        "question": RunnablePassthrough(),
    }
    | RunnableLambda(build_prompt)
    | ChatOpenAI(model="gpt-4o")
    | StrOutputParser()
)

chain_with_sources = {
    "context": retriever | RunnableLambda(parse_docs),
    "question": RunnablePassthrough(),
} | RunnablePassthrough().assign(
    response=(
        RunnableLambda(build_prompt)
        | ChatOpenAI(model="gpt-4o")
        | StrOutputParser()
    )
)

def find_next_ints_after_figure(text):
    # Find all words following the word "figure"
    matches = re.findall(r'\bfigure\b\s+(\w+)', text, flags=re.IGNORECASE)

    # Try converting each match to an integer, skip if it fails
    result = []
    for match in matches:
        try:
            result.append(int(match))
        except ValueError:
            continue
        lst = list(set(result))
        result = lst
    return result

"""# **QUERY**"""

response = chain_with_sources.invoke(
    "Explain the residual block with formula"
)

"""# **OUTPUTS**"""

print("Response:", response['response'])
for image in response['context']['images']:
    display_base64_image(image)
print("\n" + "-"*50 + "\n")

response = chain_with_sources.invoke(
    "Explain training on Imagenet"
)

print("Response:", response['response'])
for image in response['context']['images']:
    display_base64_image(image)
print("\n" + "-"*50 + "\n")

########## OUTPUT 2 #########################
if len(response['context']['images']) == 0:
  paragraph = response['response']
  next_ints = find_next_ints_after_figure(paragraph)
  if len(next_ints) > 0:
    for i in next_ints:
      target_figure = f"Figure {i}"
      if len(images)==len(captions):
        matched = next((entry for entry in captions if target_figure in entry[1]), None)
        print("Image in Page: ",matched[0])
        print(matched[1])
      else:
        print(target_figure)
        display_base64_image(images[i-1])
        print("\n" + "-"*50 + "\n")
  else:
    print("No reference figure found")

#### Summarize ######
text_summaries[0]